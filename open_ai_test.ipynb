{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package            Version\n",
      "------------------ --------\n",
      "annotated-types    0.6.0\n",
      "anyio              4.2.0\n",
      "appnope            0.1.4\n",
      "asttokens          2.4.1\n",
      "certifi            2024.7.4\n",
      "comm               0.2.2\n",
      "debugpy            1.6.7\n",
      "decorator          5.1.1\n",
      "distro             1.9.0\n",
      "exceptiongroup     1.2.2\n",
      "executing          2.0.1\n",
      "h11                0.14.0\n",
      "httpcore           1.0.2\n",
      "httpx              0.26.0\n",
      "idna               3.7\n",
      "importlib_metadata 8.0.0\n",
      "ipykernel          6.29.5\n",
      "ipython            8.26.0\n",
      "jedi               0.19.1\n",
      "jupyter_client     8.6.2\n",
      "jupyter_core       5.7.2\n",
      "matplotlib-inline  0.1.7\n",
      "nest_asyncio       1.6.0\n",
      "openai             1.25.0\n",
      "packaging          24.1\n",
      "parso              0.8.4\n",
      "pexpect            4.9.0\n",
      "pickleshare        0.7.5\n",
      "pip                24.0\n",
      "platformdirs       4.2.2\n",
      "prompt_toolkit     3.0.47\n",
      "psutil             5.9.0\n",
      "ptyprocess         0.7.0\n",
      "pure-eval          0.2.2\n",
      "pydantic           2.5.3\n",
      "pydantic_core      2.14.6\n",
      "Pygments           2.18.0\n",
      "python-dateutil    2.9.0\n",
      "pyzmq              25.1.2\n",
      "setuptools         69.5.1\n",
      "six                1.16.0\n",
      "sniffio            1.3.0\n",
      "stack-data         0.6.2\n",
      "tornado            6.4.1\n",
      "tqdm               4.66.4\n",
      "traitlets          5.14.3\n",
      "typing_extensions  4.11.0\n",
      "wcwidth            0.2.13\n",
      "wheel              0.43.0\n",
      "zipp               3.19.2\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Managing memory for Language Model (LLM) outputs is essential for efficiently handling tasks and optimizing performance. Here are a few strategies to help you maintain a short memory for LLM outputs:\n",
      "\n",
      "1. **Use Context Windows:**\n",
      "   - Limit the amount of text you feed into the model by using context windows. This way, you only input the most recent and relevant information, discarding older outputs that are no longer necessary.\n",
      "   \n",
      "2. **Token Limit Management:**\n",
      "   - Many LLMs, especially those based on architectures like GPT, have a maximum token limit for inputs. Design your system to work within these constraints, trimming the input to only include the most recent information within the token limit.\n",
      "\n",
      "3. **Summarization:**\n",
      "   - Periodically summarize longer outputs and retain only the summaries in memory. This helps in keeping the details manageable while preserving essential information.\n",
      "\n",
      "4. **Chunking:**\n",
      "   - Divide larger tasks into smaller chunks and process each independently. Only retain the output from the chunk that is relevant for further processing.\n",
      "\n",
      "5. **Sliding Window:**\n",
      "   - Implement a sliding window mechanism where you retain only a fixed amount of the latest information. As new data comes in, older data exits the window.\n",
      "\n",
      "6. **State Management:**\n",
      "   - Depending on the nature of your task, you may want to maintain explicit state information separately and refer back to it when needed. This can be in the form of key-value pairs or a structured format that stores pertinent details without holding on to all preceding text.\n",
      "\n",
      "7. **Clear Session Regularly:**\n",
      "   - Clear the memory or session after certain intervals or after completing specific tasks. This can prevent buildup of unnecessary data and help maintain focus on new inputs.\n",
      "\n",
      "8. **Selective Forgetting:**\n",
      "   - Implement policies for selective forgetting where only critical information is kept, and less important information is discarded. This can be based on heuristics or predetermined rules relevant to your use case.\n",
      "\n",
      "9. **External Memory Stores:**\n",
      "   - Use external databases or knowledge bases to store information that the model can query when needed, rather than keeping all information in active memory.\n",
      "\n",
      "10. **Efficient Log Management:**\n",
      "    - Maintain efficient logs of interactions that can be archived and referenced if needed, without keeping everything in active memory.\n",
      "\n",
      "Applying these strategies helps in managing the LLM's memory effectively, ensuring that the system remains efficient and performant while precisely retaining essential information.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set up the client\n",
    "client = AzureOpenAI()\n",
    "\n",
    "# Example of making a request\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",  # Use the model name as specified in your deployment\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you tell me how can I keep a short memory for LLM outputs?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dissertation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
