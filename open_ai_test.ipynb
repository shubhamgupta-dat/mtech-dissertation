{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                                 Version\n",
      "--------------------------------------- ---------\n",
      "aiohttp                                 3.9.5\n",
      "aiosignal                               1.3.1\n",
      "annotated-types                         0.7.0\n",
      "anyio                                   4.4.0\n",
      "appnope                                 0.1.4\n",
      "asttokens                               2.4.1\n",
      "attrs                                   23.2.0\n",
      "azure-core                              1.30.2\n",
      "azure-identity                          1.17.1\n",
      "beautifulsoup4                          4.12.3\n",
      "certifi                                 2024.7.4\n",
      "cffi                                    1.16.0\n",
      "charset-normalizer                      3.3.2\n",
      "click                                   8.1.7\n",
      "comm                                    0.2.2\n",
      "cryptography                            43.0.0\n",
      "dataclasses-json                        0.6.7\n",
      "debugpy                                 1.8.2\n",
      "decorator                               5.1.1\n",
      "Deprecated                              1.2.14\n",
      "dirtyjson                               1.0.8\n",
      "distro                                  1.9.0\n",
      "exceptiongroup                          1.2.2\n",
      "executing                               2.0.1\n",
      "frozenlist                              1.4.1\n",
      "fsspec                                  2024.6.1\n",
      "greenlet                                3.0.3\n",
      "h11                                     0.14.0\n",
      "httpcore                                1.0.5\n",
      "httpx                                   0.27.0\n",
      "idna                                    3.7\n",
      "importlib_metadata                      8.0.0\n",
      "ipykernel                               6.29.5\n",
      "ipython                                 8.26.0\n",
      "jedi                                    0.19.1\n",
      "joblib                                  1.4.2\n",
      "jupyter_client                          8.6.2\n",
      "jupyter_core                            5.7.2\n",
      "llama-cloud                             0.0.9\n",
      "llama-index                             0.10.56\n",
      "llama-index-agent-openai                0.2.9\n",
      "llama-index-cli                         0.1.12\n",
      "llama-index-core                        0.10.56\n",
      "llama-index-embeddings-azure-openai     0.1.11\n",
      "llama-index-embeddings-openai           0.1.11\n",
      "llama-index-indices-managed-llama-cloud 0.2.5\n",
      "llama-index-legacy                      0.9.48\n",
      "llama-index-llms-azure-openai           0.1.9\n",
      "llama-index-llms-openai                 0.1.26\n",
      "llama-index-multi-modal-llms-openai     0.1.8\n",
      "llama-index-program-openai              0.1.6\n",
      "llama-index-question-gen-openai         0.1.3\n",
      "llama-index-readers-file                0.1.30\n",
      "llama-index-readers-llama-parse         0.1.6\n",
      "llama-parse                             0.4.9\n",
      "marshmallow                             3.21.3\n",
      "matplotlib-inline                       0.1.7\n",
      "msal                                    1.30.0\n",
      "msal-extensions                         1.2.0\n",
      "multidict                               6.0.5\n",
      "mypy-extensions                         1.0.0\n",
      "nest_asyncio                            1.6.0\n",
      "networkx                                3.3\n",
      "nltk                                    3.8.1\n",
      "numpy                                   1.26.0\n",
      "openai                                  1.14.2\n",
      "packaging                               24.1\n",
      "pandas                                  2.1.1\n",
      "parso                                   0.8.4\n",
      "pexpect                                 4.9.0\n",
      "pickleshare                             0.7.5\n",
      "pillow                                  10.4.0\n",
      "pip                                     24.0\n",
      "platformdirs                            4.2.2\n",
      "portalocker                             2.10.1\n",
      "prompt_toolkit                          3.0.47\n",
      "psutil                                  6.0.0\n",
      "ptyprocess                              0.7.0\n",
      "pure-eval                               0.2.2\n",
      "pycparser                               2.22\n",
      "pydantic                                2.7.4\n",
      "pydantic_core                           2.18.4\n",
      "Pygments                                2.18.0\n",
      "PyJWT                                   2.8.0\n",
      "pypdf                                   4.3.0\n",
      "python-dateutil                         2.9.0\n",
      "python-dotenv                           1.0.1\n",
      "pytz                                    2024.1\n",
      "PyYAML                                  6.0.1\n",
      "pyzmq                                   26.0.3\n",
      "regex                                   2024.5.15\n",
      "requests                                2.32.3\n",
      "setuptools                              71.0.4\n",
      "six                                     1.16.0\n",
      "sniffio                                 1.3.1\n",
      "soupsieve                               2.5\n",
      "SQLAlchemy                              2.0.31\n",
      "stack-data                              0.6.2\n",
      "striprtf                                0.0.26\n",
      "tenacity                                8.5.0\n",
      "tiktoken                                0.7.0\n",
      "tornado                                 6.4.1\n",
      "tqdm                                    4.66.4\n",
      "traitlets                               5.14.3\n",
      "typing_extensions                       4.12.2\n",
      "typing-inspect                          0.9.0\n",
      "tzdata                                  2024.1\n",
      "urllib3                                 2.2.2\n",
      "wcwidth                                 0.2.13\n",
      "wheel                                   0.43.0\n",
      "wrapt                                   1.16.0\n",
      "yarl                                    1.9.4\n",
      "zipp                                    3.19.2\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Managing memory in the context of a Large Language Model (LLM) like GPT involves focusing on practical strategies to store and retrieve important information while minimizing memory usage. Here are some methods to help you keep a shorter memory while working with LLM outputs:\n",
      "\n",
      "1. **Summarization:** Use summarization techniques to condense long excerpts of information into shorter, more concise versions. This retains key points and crucial information without needing to store extensive text. Many LLMs, including variations of GPT, support summarization tasks.\n",
      "\n",
      "2. **Chunking:** Break down large pieces of information into smaller chunks or segments. This approach not only improves manageability but also allows you to load or process only the necessary chunks, thereby reducing memory usage.\n",
      "\n",
      "3. **Selective Storage:** Only store the most relevant and necessary outputs. Decide in advance what criteria make an output worth saving, such as relevance, accuracy, or frequency of use. \n",
      "\n",
      "4. **Compression:** Implement text compression algorithms to reduce the size of stored outputs. Standard text compression techniques like gzip or more advanced machine learning-based compression can be useful.\n",
      "\n",
      "5. **Forget Mechanics:** Design your system to periodically erase or overwrite older, less relevant data. This concept aligns with a sliding window approach where only the most recent and relevant information is kept in memory.\n",
      "\n",
      "6. **Use of Embeddings:** Instead of storing full-text outputs, you can store the embeddings (vector representations) of those texts. Embeddings are usually more compact and can be very effective for tasks like retrieval and similarity searches.\n",
      "\n",
      "7. **External Storage:** Use database systems or file storage solutions to offload memory usage from active processes. Store large outputs externally and retrieve them on-demand based on identifiers or keys.\n",
      "\n",
      "8. **Efficient Data Structures:** Use efficient data structures for storing information, such as hashmaps, tries, or bloom filters, which can be more memory-efficient depending on the use case.\n",
      "\n",
      "9. **Cynamic Memory Management:** If you are implementing your own LLM-based service or application, ensure that your programming language and runtime environment support dynamic memory management, and make use of garbage collection and other techniques to free up memory when no longer needed.\n",
      "\n",
      "10. **Use Proven Libraries:** Rely on well-optimized libraries and frameworks for handling and processing text data. These tools are often optimized for both speed and memory efficiency.\n",
      "\n",
      "By combining these methods, you should be able to manage and minimize memory usage effectively while still making the most of the outputs provided by your LLM.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set up the client\n",
    "client = AzureOpenAI()\n",
    "\n",
    "# Example of making a request\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",  # Use the model name as specified in your deployment\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you tell me how can I keep a short memory for LLM outputs?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dissertation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
